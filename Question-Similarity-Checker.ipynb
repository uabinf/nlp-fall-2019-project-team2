{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project initialization and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training and test data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing by cleaning data and creating embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/rc/software/Anaconda3/5.3.1/envs/DeepNLP/lib/python3.6/site-packages/ipykernel_launcher.py:71: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def get_word_list(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "ques_columns = ['question1', 'question2']\n",
    "\n",
    "for dataset in [train_df]:\n",
    "    for index, row in dataset.iterrows():\n",
    "        for question in ques_columns:\n",
    "            conversionToNumber = []\n",
    "            for word in get_word_list(row[question]):\n",
    "\n",
    "                if word in stop_words and word not in word2vec.vocab:\n",
    "                    continue\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    conversionToNumber.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    conversionToNumber.append(vocabulary[word])\n",
    "\n",
    "            dataset.set_value(index, question, conversionToNumber)\n",
    "            \n",
    "\n",
    "#initialize embedding matrix\n",
    "embeddings = 1 * np.random.randn(len(vocabulary) + 1, 300)\n",
    "embeddings[0] = 0 \n",
    "\n",
    "for word, pos in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[pos] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training, test, and validation and perform padding to the length 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 256\n",
    "training_data_size = 304290\n",
    "validation_data_size = 50000\n",
    "test_data_size = 50000\n",
    "\n",
    "X = train_df[['question1', 'question2']]\n",
    "Y = train_df['is_duplicate']\n",
    "\n",
    "X_train = X.iloc[0:training_data_size]\n",
    "X_train = {'q1': X_train.question1, 'q2': X_train.question2}\n",
    "\n",
    "Y_train = Y.iloc[0:training_data_size]\n",
    "Y_train = Y_train.values\n",
    "\n",
    "X_validation = X.iloc[training_data_size:training_data_size+validation_data_size]\n",
    "X_validation = {'q1': X_validation.question1, 'q2': X_validation.question2}\n",
    "\n",
    "Y_validation = Y.iloc[training_data_size:training_data_size+validation_data_size]\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "X_test = X.iloc[training_data_size+validation_data_size:training_data_size+validation_data_size+test_data_size]\n",
    "X_test = {'q1': X_test.question1, 'q2': X_test.question2}\n",
    "\n",
    "Y_test = Y.iloc[training_data_size+validation_data_size:training_data_size+validation_data_size+test_data_size]\n",
    "Y_test = Y_test.values\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Perform Padding to make equal to max_seq_length\n",
    "for dataset, pos in itertools.product([X_train, X_validation, X_test], ['q1', 'q2']):\n",
    "    dataset[pos] = pad_sequences(dataset[pos], maxlen=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 104200 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "104200/104200 [==============================] - 790s 8ms/step - loss: 0.3334 - acc: 0.6447 - val_loss: 0.2870 - val_acc: 0.6656\n",
      "Epoch 2/2\n",
      "104200/104200 [==============================] - 787s 8ms/step - loss: 0.2643 - acc: 0.6813 - val_loss: 0.2469 - val_acc: 0.6900\n",
      "Total training time is 0:26:19.639511\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Lambda\n",
    "import keras.backend as K\n",
    "import h5py\n",
    "from keras.optimizers import Adadelta, Adam\n",
    "\n",
    "n_hidden = 50\n",
    "batch_size = 64\n",
    "n_epoch = 10\n",
    "\n",
    "def manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "embedding_layer = Embedding(len(embeddings), 300, weights=[embeddings], input_length=max_seq_length, trainable=False)\n",
    "\n",
    "input_question1 = Input(shape=(max_seq_length,), dtype='int32')\n",
    "input_question2 = Input(shape=(max_seq_length,), dtype='int32')\n",
    "encoded_question1 = embedding_layer(input_question1)\n",
    "encoded_question2 = embedding_layer(input_question2)\n",
    "\n",
    "lstm1 = LSTM(n_hidden)\n",
    "lstm2 = LSTM(n_hidden)\n",
    "output_question1 = lstm1(encoded_question1)\n",
    "output_question2 = lstm2(encoded_question2)\n",
    "\n",
    "malstm_distance = Lambda(lambda x: manhattan_distance(x[0], x[1]), lambda x: (x[0][0], 1))([output_question1, output_question2])\n",
    "\n",
    "# Build the model\n",
    "lstm_model = Model([input_question1, input_question2], [malstm_distance])\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer=Adam(clipnorm=1.25), metrics=['accuracy'])\n",
    "\n",
    "training_start_time = time() \n",
    "\n",
    "malstm_trained = lstm_model.fit([X_train['q1'], X_train['q2']], Y_train, batch_size=batch_size, epochs=n_epoch,\n",
    "                            validation_data=([X_validation['q1'], X_validation['q2']], Y_validation))\n",
    "\n",
    "\n",
    "required_time = datetime.timedelta(seconds=time()-training_start_time)\n",
    "print(\"Total training time is {}\".format(required_time))\n",
    "\n",
    "lstm_model.save('model/train.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Model([input_question1, input_question2], [malstm_distance])\n",
    "lstm_model.load_weights('model/train.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate and predict on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_validation = lstm_model.predict([X_validation['q1'], X_validation['q2']], batch_size=batch_size)\n",
    "predictions_validation = [item for sublist in predictions_validation for item in sublist]\n",
    "\n",
    "predictions_test = lstm_model.predict([X_test['q1'], X_test['q2']], batch_size=batch_size)\n",
    "predictions_test = [item for sublist in predictions_test for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3259\n"
     ]
    }
   ],
   "source": [
    "predicted_values = []\n",
    "\n",
    "for prediction in predictions_test:\n",
    "    if prediction > 0.5:\n",
    "        predicted_values.append(True)\n",
    "    else:\n",
    "        predicted_values.append(False)\n",
    "\n",
    "correct_predictions = sum(predicted_values == (Y_test == 1))\n",
    "print(correct_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[3020  142]\n",
      " [1599  239]]\n",
      "Accuracy:  0.6518\n",
      "Recall:  0.13003264417845484\n",
      "Precision:  0.6272965879265092\n",
      "F1 score:  0.21541234790446145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "# Compute confusion matrix\n",
    "y_test = Y_test\n",
    "y_pred = predicted_values\n",
    "y_pred_prob = predictions_test\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion matrix')\n",
    "print(cm)\n",
    "\n",
    "rec = skm.recall_score(y_test, y_pred)\n",
    "prec = skm.precision_score(y_test, y_pred)\n",
    "f1 = skm.f1_score(y_test, y_pred)\n",
    "acc = skm.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Recall: \", rec)\n",
    "print(\"Precision: \", prec)\n",
    "print(\"F1 score: \", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/rc/software/Anaconda3/5.3.1/envs/DeepNLP/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  app.launch_new_instance()\n",
      "/share/apps/rc/software/Anaconda3/5.3.1/envs/DeepNLP/lib/python3.6/site-packages/ipykernel_launcher.py:30: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'How does the Surface Pro himself 4 compare with iPad Pro?'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9b5bf0fbc84a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'q1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtestset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mpredictions_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmalstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/rc/software/Anaconda3/5.3.1/envs/DeepNLP/lib/python3.6/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[0;32m/share/apps/rc/software/Anaconda3/5.3.1/envs/DeepNLP/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'How does the Surface Pro himself 4 compare with iPad Pro?'"
     ]
    }
   ],
   "source": [
    "for testset in [test_df[0:10]]:\n",
    "    for index, row in testset.iterrows():\n",
    "        \n",
    "        conversionToNumber = []\n",
    "        for word in get_word_list(row['question1']):\n",
    "\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "\n",
    "            if word not in vocabulary:\n",
    "                conversionToNumber.append(0)\n",
    "            else:\n",
    "                #print(vocabulary[word])\n",
    "                conversionToNumber.append(vocabulary[word])\n",
    "\n",
    "        testset.set_value(index, question, conversionToNumber)\n",
    "        \n",
    "        conversionToNumber = []\n",
    "        for word in get_word_list(row['question2']):\n",
    "\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "\n",
    "            if word not in vocabulary:\n",
    "                conversionToNumber.append(0)\n",
    "            else:\n",
    "                #print(word)\n",
    "                conversionToNumber.append(vocabulary[word])\n",
    "\n",
    "        testset.set_value(index, question, conversionToNumber)\n",
    "            \n",
    "test_X = test_df[['question1', 'question2']]\n",
    "\n",
    "test_X = test_X.iloc[0:10]\n",
    "\n",
    "test_X = {'q1': test_X.question1, 'q2': test_X.question2} \n",
    "\n",
    "for testset, pos in itertools.product([test_X], ['q1', 'q2']):\n",
    "    testset[pos] = pad_sequences(testset[pos], maxlen=max_seq_length)\n",
    "    \n",
    "predictions_test = lstm_model.predict([test_X['q1'], test_X['q2']], batch_size=batch_size)\n",
    "predictions_test = [item for sublist in predictions_test for item in sublist]\n",
    "\n",
    "print(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DeepNLP]",
   "language": "python",
   "name": "conda-env-DeepNLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
